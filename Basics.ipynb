{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensorflow**\n",
        "## Tokenization\n",
        "###import libraries and API"
      ],
      "metadata": {
        "id": "tDtRUcvakNZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "z_mYHa-mmXGG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#array of sentences\n",
        "sentences = [\n",
        "    'I like pretty clouds',\n",
        "    'I like pretty sky',\n",
        "    'You like pretty clouds!',\n",
        "    'Do you think the sky looks marvelous?'\n",
        "]\n"
      ],
      "metadata": {
        "id": "ak31x863m0qR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num_words maximum words to keep if there were repeatative words\n",
        "#oov_token : tokenizer creates a token for oov, \n",
        "#replace words that ot doest recognize(out of vocab)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "O6Rs9fIenUtX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* (advanced soln): Ragged tensor\n",
        "* (simpler soln): **Padding** \n",
        "\n",
        "Handles the sentences of different length "
      ],
      "metadata": {
        "id": "DEhNVo_-IcQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sequencing"
      ],
      "metadata": {
        "id": "D5flAka9NqpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sequences of number from sentences\n",
        "#it creates sequences of tokens representing each sentence\n",
        "#sequence lenthg = sentence length, if not, use padding for dif len\n",
        "#padding ='post' means zeros after the sentence\n",
        "#pad_sequence(truncating ='post'/'pre' ) if sentences are longer than max length \n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences, padding='post', truncating='post',maxlen=5)\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "print(padded)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjDjq9F0OhJA",
        "outputId": "ca203321-83f9-4a0a-8910-0d148e8a79f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'like': 2, 'pretty': 3, 'i': 4, 'clouds': 5, 'sky': 6, 'you': 7, 'do': 8, 'think': 9, 'the': 10, 'looks': 11, 'marvelous': 12}\n",
            "[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5], [8, 7, 9, 10, 6, 11, 12]]\n",
            "[[ 4  2  3  5  0]\n",
            " [ 4  2  3  6  0]\n",
            " [ 7  2  3  5  0]\n",
            " [ 8  7  9 10  6]]\n"
          ]
        }
      ]
    }
  ]
}